<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="de" xml:lang="de"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Lineare Klassifikation – Machine Learning verstehen statt nur anwenden</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04_pipeline.html" rel="next">
<link href="./02_linreg.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e83ed3172a0847c3b1a4e9a3dee0bd66.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Keine Treffer",
    "search-matching-documents-text": "Treffer",
    "search-copy-link-title": "Link in die Suche kopieren",
    "search-hide-matches-text": "Zusätzliche Treffer verbergen",
    "search-more-match-text": "weitere Treffer in diesem Dokument",
    "search-more-matches-text": "weitere Treffer in diesem Dokument",
    "search-clear-button-title": "Zurücksetzen",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Abbrechen",
    "search-submit-button-title": "Abschicken",
    "search-label": "Suchen"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./classical.html">Klassisches Machine Learning</a></li><li class="breadcrumb-item"><a href="./03_linclass.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Lineare Klassifikation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Seitenleiste umschalten" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Suchen" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning verstehen statt nur anwenden</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Suchen"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Über das Buch</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Einführung</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Klassisches Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_linreg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Lineare Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_linclass.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Lineare Klassifikation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_pipeline.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">ML Pipeline</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Entscheidungsbäume</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_ensemble.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Ensembles</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./deep.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_ann.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Artificial Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_rnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Recurrent Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Transformers</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quellenverzeichnis</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Anhang</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Abschnitt umschalten">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Mathe- und Statistik-Grundlagen</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">R und Python</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Inhaltsverzeichnis</h2>
   
  <ul>
  <li><a href="#logistische-regression" id="toc-logistische-regression" class="nav-link active" data-scroll-target="#logistische-regression"><span class="header-section-number">3.1</span> Logistische Regression</a>
  <ul class="collapse">
  <li><a href="#output-variable" id="toc-output-variable" class="nav-link" data-scroll-target="#output-variable"><span class="header-section-number">3.1.1</span> Output-Variable</a></li>
  <li><a href="#modellspezifikation" id="toc-modellspezifikation" class="nav-link" data-scroll-target="#modellspezifikation"><span class="header-section-number">3.1.2</span> Modellspezifikation</a></li>
  <li><a href="#modelltraining" id="toc-modelltraining" class="nav-link" data-scroll-target="#modelltraining"><span class="header-section-number">3.1.3</span> Modelltraining</a></li>
  <li><a href="#decision-boundaries" id="toc-decision-boundaries" class="nav-link" data-scroll-target="#decision-boundaries"><span class="header-section-number">3.1.4</span> Decision Boundaries</a></li>
  </ul></li>
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes"><span class="header-section-number">3.2</span> Naive Bayes</a>
  <ul class="collapse">
  <li><a href="#modellspezifikation-1" id="toc-modellspezifikation-1" class="nav-link" data-scroll-target="#modellspezifikation-1"><span class="header-section-number">3.2.1</span> Modellspezifikation</a></li>
  <li><a href="#modelltraining-1" id="toc-modelltraining-1" class="nav-link" data-scroll-target="#modelltraining-1"><span class="header-section-number">3.2.2</span> Modelltraining</a></li>
  <li><a href="#spam-filter" id="toc-spam-filter" class="nav-link" data-scroll-target="#spam-filter"><span class="header-section-number">3.2.3</span> Spam Filter</a></li>
  </ul></li>
  <li><a href="#k-nearest-neighbors" id="toc-k-nearest-neighbors" class="nav-link" data-scroll-target="#k-nearest-neighbors"><span class="header-section-number">3.3</span> K-Nearest Neighbors</a></li>
  <li><a href="#sec-logregR" id="toc-sec-logregR" class="nav-link" data-scroll-target="#sec-logregR"><span class="header-section-number">3.4</span> Modelle in R</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./classical.html">Klassisches Machine Learning</a></li><li class="breadcrumb-item"><a href="./03_linclass.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Lineare Klassifikation</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-linclass" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Lineare Klassifikation</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Nachdem wir uns in <a href="02_linreg.html" class="quarto-xref"><span>Kapitel 2</span></a> bereits ausführlich mit dem Regressionsproblem befasst haben, lernen wir hier nun das andere grosse Supervised Learning Problem kennen: das <strong>Klassifikationsproblem</strong>. In der Praxis sind Klassifikationsprobleme fast häufiger anzutreffen als Regressionsprobleme.</p>
<p>Wir fokussieren hier erstmal auf das <strong>binäre Klassifikationsproblem</strong>, bei dem unser Ziel ist vorherzusagen, ob eine Beobachtung der Kategorie 1 (<span class="math inline">\(y_i=1\)</span>) oder der Kategorie 0 (<span class="math inline">\(y_i=0\)</span>) angehört.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Unsere Output-Variable ist hier also nicht mehr quantitativer Natur, sondern <strong>qualitativ</strong> (d.h. kategorisch) und hat zwei mögliche Kategorien (oder Klassen).</p>
<p>Wir werden uns in diesem Kapitel drei Modelle für das Klassifikationsproblem anschauen:</p>
<ul>
<li>Logistische Regression<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> (LR)</li>
<li>Naive Bayes (NB)</li>
<li>K-Nearest Neighbors (KNN)</li>
</ul>
<p><strong>Wichtig</strong>: der Titel des Kapitels besagt, dass wir uns hier <em>lineare</em> Klassifikationsmodelle anschauen. Das stimmt nicht ganz, denn während das logistische Regressionsmodell und Naive Bayes lineare Modelle sind, ist KNN ein sehr flexibles, nicht-lineares Modell.</p>
<section id="logistische-regression" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="logistische-regression"><span class="header-section-number">3.1</span> Logistische Regression</h2>
<p>Die logistische Regression ist das Gegenstück zur linearen Regression für das Klassifikationsproblem. Es ist ein einfaches Modell, das oft als guter Ausgangspunkt für jegliche Klassifikationsprobleme dient. Das heisst, es macht meist als <strong>erstes Modell</strong> ein logistisches Regressionsmodell zu trainieren.</p>
<section id="output-variable" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="output-variable"><span class="header-section-number">3.1.1</span> Output-Variable</h3>
<p>Die Output-Variable bzw. das Label im <strong>binären</strong> Klassifikationsproblem ist eine kategorische Variable mit zwei möglichen Kategorien (oder Klassen). Wie oben erwähnt nimmt die Output-Variable den Wert 1 an, also <span class="math inline">\(y_i=1\)</span>, falls eine Beobachtung der Kategorie 1 angehört und sonst den Wert 0, <span class="math inline">\(y_i=0\)</span>. Typischerweise geben wir den Wert 1 der Kategorie, die uns wirklich interessiert (z.B. Spam, Zahlungsunfähigkeit, Patient hat eine Krankheit).</p>
<p>In folgender Abbildung ist ein Klassifikationsproblem mit 5 Beobachtungen dargestellt. Die drei blauen Datenpunkte gehören der Kategorie 1 an, während die zwei roten Datenpunkte der Kategorie 0 angehören. Wir werden weiter unten auf dieses Problem zurückkommen.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/class-prob-1.png" class="img-fluid figure-img" style="width:60.0%" alt="Einfaches Klassifikationsbeispiel."></p>
<figcaption>Abgebildet ist die Ausgangslage eines einfachen Klassifikationsproblems mit einer Input-Variable (eingezeichnet auf der x-Achse) und der Output-Variable (eingezeichnet auf der y-Achse).</figcaption>
</figure>
</div>
</div>
</div>
<p>Bei den meisten Klassifikationsmodellen wird <em>nicht</em> direkt der Output <span class="math inline">\(y_i\)</span> modelliert (wie das bei den Regressionsproblemen der Fall ist), sondern die (bedingte) <strong>Wahrscheinlichkeit</strong>, dass <span class="math inline">\(y_i=1\)</span>, gegeben irgendwelche Input-Datenwerte. Mathematisch schreiben wir diese Wahrscheinlichkeit als <span class="math inline">\(p(y_i = 1 \mid \mathbf{x}_i)\)</span>. Alle unsere Modelle hier werden versuchen, möglichst genaue Schätzung für diese bedingte Wahrscheinlichkeit zu finden.</p>
<section id="frage" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="frage">Frage</h4>
<p>Warum modellieren wir <span class="math inline">\(p(y_i = 1 \mid \mathbf{x}_i)\)</span> nicht mit dem linearen Regressionsmodell?</p>
<ol type="a">
<li>Je nach Input-Daten könnten wir negative Wahrscheinlichkeiten oder Wahrscheinlichkeiten grösser als 1 erhalten.</li>
<li>Weil wir für die Klassifikation nie ein lineares Modell schätzen.</li>
<li>Das lineare Regressionsmodell funktioniert gar nicht in diesem Fall.</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Die richtige Antwort ist <strong>a</strong>.</p>
<p>Antwort b. stimmt nicht, weil es sich beim logistischen Regressionsmodell ja eben auch um ein lineares Modell handelt.</p>
<p>Antwort c.&nbsp;stimmt nicht, weil das lineare Regressionsmodell technisch problemlos rechenbar ist, aber eben zum unerwünschten Resultat (beschrieben in a.) führt.</p>
</div>
</div>
</div>
<p>Nun wundern Sie sich vielleicht: wenn uns unser Modell eine Schätzung für <span class="math inline">\(p(y_i = 1 \mid \mathbf{x}_i)\)</span> zurück gibt, wie mache ich damit eine Vorhersage?</p>
<p>Im einfachsten Fall ist unsere Vorhersage <span class="math inline">\(\hat{y}_i=1\)</span> (also Kategorie 1), falls die geschätzte Wahrscheinlichkeit <span class="math inline">\(p(y_i = 1 \mid \mathbf{x}_i) &gt; 50\%\)</span>, und sonst <span class="math inline">\(\hat{y}_i=0\)</span>. Hier haben wir einen <strong>Threshold</strong> von 50% gewählt, aber grundsätzlich können wir auch andere Thresholds wählen (z.B. 20% oder 85%), um von den geschätzten Wahrscheinlichkeiten eine Vorhersage abzuleiten. Wir werden uns in <a href="04_pipeline.html" class="quarto-xref"><span>Kapitel 4</span></a> genauer mit dem optimalen Threshold befassen.</p>
</section>
</section>
<section id="modellspezifikation" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="modellspezifikation"><span class="header-section-number">3.1.2</span> Modellspezifikation</h3>
<p>Bevor wir das <strong>logistische</strong> Regressionsmodell spezifizieren, erinnern wir uns ganz kurz an die Modellspezifikation des <strong>linearen</strong> Regressionsmodells:</p>
<p><span class="math display">\[
f(\mathbf{x}_i) = w_0 + w_1 \cdot x_{i1} + w_2 \cdot x_{i2} + \ldots + w_p \cdot x_{ip}
\]</span> Diese gewichtete Summe der Input-Variablenwerte wird auch bei der logistischen Regression eine wichtige Rolle spielen. Wir haben in der ersten Frage zu diesem Kapitel bereits gesehen, dass folgende Modellgleichung leider nicht eine gute Idee ist:</p>
<p><span class="math display">\[
p(y_i = 1 \mid \mathbf{x}_i) = w_0 + w_1 \cdot x_{i1} + w_2 \cdot x_{i2} + \ldots + w_p \cdot x_{ip}
\]</span> Warum nicht? Weil die gewichtete Summe je nach Input-Daten mal negativ sein kann oder auch grösser als 1. Gleichzeitig wissen wir aus der Wahrscheinlichkeitstheorie, dass eine Wahrscheinlichkeit immer zwischen 0 und 1 liegen muss. Folgende Abbildung zeigt, dass das lineare Regressionsmodell nur innerhalb der gestrichelten, vertikalen Linien eine Wahrscheinlichkeit zwischen 0 und 1 zurückgibt.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/class-prob-lin-reg-1.png" class="img-fluid figure-img" style="width:60.0%" alt="Lin. Reg. für einfaches Klassifikationsbeispiel."></p>
<figcaption>Was passiert, wenn wir ein einfaches lineares Regressionsmodell schätzen für unser kleines Klassifikationsproblem.</figcaption>
</figure>
</div>
</div>
</div>
<p>Wir müssen also die gewichtete Summe in eine mathematische Funktion “verpacken”, so dass die Outputs dieser Funktion immer zwischen 0 und 1 liegen. Die <strong>Sigmoid Funktion</strong> (auch logistische Funktion genannt) tut genau das. Sie nimmt irgendeinen (reellen) Input <span class="math inline">\(z\)</span> und gibt immer einen Wert zwischen 0 und 1 zurück. Die Funktion sieht folgendermassen aus:</p>
<p><span class="math display">\[
g(z) = \frac{e^z}{1+e^z}=\frac{1}{1+e^{-z}}
\]</span></p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Vorsicht</span>Zwei Formen der Sigmoid Funktion (optional)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Wir wollen uns hier anschauen, wie wir von der einen zur anderen Form der Sigmoid Funktion kommen:</p>
<p><span class="math display">\[
\frac{e^z}{1+e^z} \cdot \frac{e^{-z}}{e^{-z}} = \frac{e^z \cdot e^{-z}}{(1+e^z) \cdot e^{-z}} = \frac{e^0}{e^{-z} + e^0} = \frac{1}{1+e^{-z}}
\]</span> Wir multiplizieren die erste Form der Sigmoid Funktion in einem ersten Schritt mit <span class="math inline">\(\frac{e^{-z}}{e^{-z}}\)</span>. Das ist erlaubt, weil dieser Ausdruck ja nichts anderes als 1 ist und durch diese Multiplikation die Sigmoid Funktion nicht verändert wird. Der Rest ist dann einfaches, algebraisches Umformen.</p>
</div>
</div>
</div>
<p>Für jeden Wert von <span class="math inline">\(z\)</span> gibt uns die Funktion <span class="math inline">\(g(z)\)</span> einen Wert zwischen 0 und 1 zurück. Grafisch sieht die Sigmoid Funktion folgendermassen aus:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/sigm-1.png" class="img-fluid figure-img" style="width:60.0%" alt="Sigmoid Funktion"></p>
<figcaption>Die Form der Sigmoid Funktion. Per Definition hat die Funktion bei einem Input-Wert von 0 den Wert 0.5. Das ist einfach aus der Formel oben ersichtlich, wenn man bedenkt, dass <span class="math inline">\(e^{0} = 1\)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<section id="frage-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="frage-1">Frage</h4>
<p>Was ist der Output der Sigmoid Funktion, wenn <span class="math inline">\(z=2.6\)</span>?</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Die korrekte Antwort ist 0.9309.</p>
</div>
</div>
</div>
<p>Nun können wir alles zusammensetzen, was ganz einfach bedeutet, dass wir nun anstelle von <span class="math inline">\(z\)</span> die gewichtete Summe der Input-Variablen in die Sigmoid Funktion einsetzen. Das logistische Regressionsmodell sieht dementsprechend dann wie folgt aus:</p>
<p><span class="math display">\[\begin{align}
p(y_i = 1 \mid \mathbf{x}_i) &amp;= \frac{1}{1+e^{-(w_0 + w_1 \cdot x_{i1} + w_2 \cdot x_{i2} + \ldots + w_p \cdot x_{ip})}}\\
&amp;= \frac{1}{1+e^{-(\mathbf{w}' \mathbf{x_i})}}
\end{align}\]</span></p>
<p>Im nächsten Abschnitt wollen wir herausfinden, wie die Parameter des Modells geschätzt werden.</p>
</section>
</section>
<section id="modelltraining" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="modelltraining"><span class="header-section-number">3.1.3</span> Modelltraining</h3>
<p>Modelltraining bedeutet auch hier nichts anderes, als dass wir die Parameter des Modells möglichst optimal schätzen wollen. Sie haben in <a href="02_linreg.html" class="quarto-xref"><span>Kapitel 2</span></a> (Perspektive 1 des Modelltrainings bei der lin. Regression) bereits gelernt, dass wir in der Phase des Modelltrainings eine <strong>Kostenfunktion</strong> aufstellen, die es dann zu minimieren gilt. Doch wie soll eine Kostenfunktion für das Klassifikationsproblem aussehen?</p>
<p>Im <strong>Idealfall</strong> gibt unser Modell eine Wahrscheinlichkeit von 0 aus für alle Beobachtungen, bei denen <span class="math inline">\(y_i=0\)</span>, während für alle Beobachtungen, wo <span class="math inline">\(y_i=1\)</span>, eine Wahrscheinlichkeit von 1 zurückgegeben wird. In diesem Idealfall sollte die Kostenfunktion 0 betragen. Wie können wir das mathematisch abbilden?</p>
<p>Die “Kosten” für eine <strong>einzelne Beobachtung</strong> mit dem (wahren) Outputwert <span class="math inline">\(y_i=1\)</span> schreiben wir als <span class="math inline">\(-\text{log}(p(y_i = 1 \mid \mathbf{x}_i))\)</span>. Für Beobachtungen mit dem (wahren) Label <span class="math inline">\(y_i=0\)</span> sind die “Kosten” dementsprechend <span class="math inline">\(-\text{log}(1-p(y_i = 1 \mid \mathbf{x}_i))\)</span>. Warum das Sinn macht, sehen Sie anhand folgender Abbildung:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/indiv-cost-1.png" class="img-fluid figure-img" style="width:100.0%" alt="Individuelle Kosten bei der log. Regression."></p>
<figcaption>Links: individuelle Kostenfunktion für eine Beobachtung mit wahrem Label der Kategorie 1, also <span class="math inline">\(y_i=1\)</span>. Rechts: individuelle Kostenfunktion für eine Beobachtung mit wahrem Label der Kategorie 0, also <span class="math inline">\(y_i=0\)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>In der linken Abbildung sehen Sie einen Kostenwert, der immer kleiner wird, je grösser die Wahrscheinlichkeit <span class="math inline">\(p(y_i = 1 \mid \mathbf{x}_i)\)</span>. Rechts sehen wir genau das Gegenteil: je grösser diese Wahrscheinlichkeit, desto grösser die Kosten. Wir wollen in diesem Fall, dass die Wahrscheinlichkeit <span class="math inline">\(p(y_i = 1 \mid \mathbf{x}_i)\)</span> so klein wie möglich ist.</p>
<p>Wir haben nun die Kosten für <strong>einzelne Beobachtungen</strong> angeschaut und gesehen, dass die Kosten je nach wahrem Label <span class="math inline">\(y_i\)</span> unterschiedlich sind. Die <strong>Gesamtkostenfunktion</strong> ist der Durchschnitt über die individuellen Kosten (gemittelt über alle Beobachtungen im Trainingsdatensatz). Mathematisch sieht das Ganze folgendermassen aus:</p>
<p><span class="math display">\[
J = -\frac{1}{n}\sum_{i=1}^n \left[y_i \cdot \log\left(p(y_i = 1 \mid \mathbf{x}_i)\right) + (1-y_i) \cdot \log\left(1-p(y_i = 1 \mid \mathbf{x}_i)\right)\right]
\]</span></p>
<p>Überlegen Sie sich kurz, warum diese Berechnung Sinn macht. Wenn die <span class="math inline">\(i\)</span>-te Beobachtung das Label <span class="math inline">\(y_i=1\)</span> hat, dann entfällt der zweite Teil in den eckigen Klammern, weil in diesem Fall <span class="math inline">\((1-y_i)=0\)</span> ist. Wenn die <span class="math inline">\(i\)</span>-te Beobachtung hingegen das Label <span class="math inline">\(y_i=0\)</span> hat, dann entfällt der erste Teil in den eckigen Klammern, weil die individuellen Kosten mit 0 multipliziert werden. Wir haben hier also eine kompakte mathematische Form, um die Gesamtkostenfunktion aufzuschreiben. In der Praxis wird diese Kostenfunktion häufig <strong>log-loss</strong> genannt.</p>
<p>Wir suchen also nun die Parameterwerte <span class="math inline">\(w_0, w_1, \dots\)</span>, welche die obige Kostenfunktion minimieren. Leider gibt es <em>keine</em> analytische Lösung dafür. Wir können aber die optimalen Parameterwerte mit anderen (iterativen) Optimierungsmethoden finden. In <code>R</code> und <code>Python</code> sind diese Methoden effizient implementert (<a href="#sec-logregR" class="quarto-xref">Abschnitt&nbsp;<span>3.4</span></a>), so dass wir die Parameter, welche diese Kostenfunktion minimieren, oft in wenigen Millisekunden finden.</p>
<p>Die folgende Abbildung zeigt das trainierte Modell:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/class-prob-log-reg-1.png" class="img-fluid figure-img" style="width:60.0%" alt="Log. Reg. für einfaches Klassifikationsbeispiel."></p>
<figcaption>Trainiertes logistisches Regressionsmodell für unser einfaches Klassifikationsbeispiel.</figcaption>
</figure>
</div>
</div>
</div>
<p>Die optimalen Parameter sind <span class="math inline">\(w_0 = 0.15\)</span> und <span class="math inline">\(w_1 = 0.99\)</span>. Wir sehen, dass die gefittete Sigmoid Funktion nicht bei <span class="math inline">\(x_i=0\)</span> den Wert 0.5 hat, sondern etwas vorher (nämlich bei -0.15). Der Parameter <span class="math inline">\(w_0\)</span> gibt uns also in einem gewissen Sinn die horizontale “Flexibilität” beim Fitten des Modells bzw. der Funktion. Unser finales Modell sieht also wie folgt aus:</p>
<p><span class="math display">\[
p(y_i = 1 \mid x_i) = \frac{1}{1+e^{-(0.15 + 0.99 \cdot x_{i})}}
\]</span></p>
<section id="frage-2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="frage-2">Frage</h4>
<p>Was ist die Vorhersage unseres Modells für den Input-Variablenwert <span class="math inline">\(x_i=-2\)</span>?</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Die korrekte Antwort ergibt sich durch Einsetzen des Werts in das trainierte Modell:</p>
<p><span class="math display">\[
p(y_i = 1 \mid x_i) = \frac{1}{1+e^{-(0.15 + 0.99 \cdot (-2))}} = \frac{1}{1+e^{-(-1.83)}} = 0.14
\]</span> Gemäss unserem Modell ist die Wahrscheinlichkeit, dass <span class="math inline">\(y_i = 1\)</span> lediglich 14% für diesen Input-Variablenwert.</p>
</div>
</div>
</div>
<p>Zwei letzte wichtige Punkte in diesem Abschnitt:</p>
<ul>
<li>Wie bei der linearen Regression verwenden wir bei der logistischen Regression typischerweise <strong>Regularisierung</strong>, um dem Overfitting entgegenzuwirken. Von der Idee her funktioniert es genau gleich wie bei der linearen Regression.</li>
<li>Die durchschnittliche Wahrscheinlichkeit, welche unser trainiertes Modell auf dem Trainingsdatensatz schätzt, entspricht bei der logistischen Regression jeweils genau dem Anteil an Beobachtungen (im Trainingsdatensatz) mit Label <span class="math inline">\(y_i=1\)</span>.</li>
</ul>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Vorsicht</span>Herleitung des zweiten Punkts (optional)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Wir können das logistische Regressionsmodell schreiben als</p>
<p><span class="math display">\[
p(y_i = 1 \mid x_i) = g(\mathbf{w}' \mathbf{x_i})\,,
\]</span> wobei <span class="math inline">\(g()\)</span> die Sigmoid Funktion bezeichnet.</p>
<p>Die erste Ableitung der <em>allgemeinen</em> Form der Sigmoid Funktion <span class="math inline">\(g(z)\)</span> hat folgende einfache Form:</p>
<p><span class="math display">\[
\frac{d\,g(z)}{dz} = g(z) \cdot (1 - g(z))
\]</span></p>
<p>Nun können wir die Ableitung der Kostenfunktion nach den Modellparameter <span class="math inline">\(\mathbf{w}\)</span> rechnen. Dazu verwenden wir die obigen zwei Resultate, die Tatsache, dass die Ableitung der Logarithmusfunktion <span class="math inline">\(\log(z)=\frac{1}{z}\)</span> ist und die bekannte <strong>Kettenregel</strong> für Ableitungen:</p>
<p><span class="math display">\[
\begin{split}
\frac{\partial J}{\partial \mathbf{w}}
&amp;= -\frac{1}{n} \sum_{i=1}^n \Bigl[
y_i \cdot \frac{1}{g(\mathbf{w}' \mathbf{x_i})} \cdot g(\mathbf{w}' \mathbf{x_i})
\cdot (1 - g(\mathbf{w}' \mathbf{x_i})) \cdot \mathbf{x}_i \\
&amp;\qquad + (1- y_i) \cdot \frac{1}{1 - g(\mathbf{w}' \mathbf{x_i})} \cdot (-1)
\cdot g(\mathbf{w}' \mathbf{x_i}) \cdot (1 - g(\mathbf{w}' \mathbf{x_i}))
\cdot \mathbf{x}_i
\Bigr]
\\
&amp;= -\frac{1}{n} \sum_{i=1}^n \left[y_i \cdot (1 - g(\mathbf{w}' \mathbf{x_i})) \cdot \mathbf{x}_i - (1- y_i) \cdot g(\mathbf{w}' \mathbf{x_i}) \cdot \mathbf{x}_i\right]\\
&amp;= -\frac{1}{n} \sum_{i=1}^n \left[y_i \cdot \mathbf{x}_i - y_i \cdot \mathbf{x}_i \cdot g(\mathbf{w}' \mathbf{x_i}) -  \mathbf{x}_i \cdot g(\mathbf{w}' \mathbf{x_i}) + y_i \cdot \mathbf{x}_i \cdot g(\mathbf{w}' \mathbf{x_i})\right]\\
&amp;= -\frac{1}{n} \sum_{i=1}^n \left[y_i \cdot \mathbf{x}_i -  \mathbf{x}_i \cdot g(\mathbf{w}' \mathbf{x_i})\right]\\
&amp;= -\frac{1}{n} \sum_{i=1}^n \left[(y_i - g(\mathbf{w}' \mathbf{x_i})) \cdot \mathbf{x}_i\right]\\
\end{split}
\]</span> Obschon obige Umformung der Ableitung etwas wild daherkommt, sehen Sie vielleicht, dass wir mit simplen algebraischen Umformungen zu einem einfachen Ausdruck der ersten Ableitung in der letzten Zeile kommen.</p>
<p>Wir sehen nun folgende Elemente in dieser Ableitung:</p>
<ul>
<li>Der Term <span class="math inline">\((y_i - g(\mathbf{w}' \mathbf{x_i}))\)</span> ist nichts anderes als eine Zahl (= Skalar), die den Fehler für die <span class="math inline">\(i\)</span>-te Beobachtung beschreibt. Wie stark weicht die Vorhersage des Modells <span class="math inline">\(g(\mathbf{w}' \mathbf{x_i})\)</span> vom wahren Output <span class="math inline">\(y_i\)</span> ab?</li>
<li><span class="math inline">\(\mathbf{x_i}\)</span> hingegen ist ein Vektor, weshalb die Ableitung oben im Prinzip die Ableitungen nach allen <span class="math inline">\(p\)</span> Modellparameter enthält.</li>
</ul>
<p>Aus letzterem Punkt folgt, dass wenn wir nun die Ableitung gleich Null setzen, um die optimalen Parameter zu finden, wir eigentlich ein Gleichungssystem mit <span class="math inline">\(p\)</span> Gleichungen haben. Wir schauen uns hier nun aber nur die Ableitung nach der Konstante <span class="math inline">\(w_0\)</span> an, für die <span class="math inline">\(\mathbf{x}_i\)</span> an erster Stelle eine 1 enthält:</p>
<p><span class="math display">\[
\begin{split}
-\frac{1}{n} \sum_{i=1}^n (y_i - g(\mathbf{w}' \mathbf{x_i})) \cdot 1 &amp;= 0\\
-\frac{1}{n} \sum_{i=1}^n y_i + \frac{1}{n} \sum_{i=1}^n g(\mathbf{w}' \mathbf{x_i}) &amp;= 0\\
\frac{1}{n} \sum_{i=1}^n g(\mathbf{w}' \mathbf{x_i}) &amp;= \frac{1}{n} \sum_{i=1}^n y_i\\
\end{split}
\]</span></p>
<p>Wow, in der letzten Zeile sehen wir nun genau das gesuchte Resultat. Am Optimum (der Modellparameter) <strong>muss gelten</strong>, dass der Durchschnitt über die <span class="math inline">\(y\)</span>-Werte genau dem Durchschnitt über die Modelloutputs entspricht.</p>
</div>
</div>
</div>
</section>
</section>
<section id="decision-boundaries" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="decision-boundaries"><span class="header-section-number">3.1.4</span> Decision Boundaries</h3>
<p>Wir haben nun gesehen, wie das logistische Regressionsmodell aussieht, wie es trainiert wird und wie es auf unser kleines Beispiel angewendet wird. Doch Ihnen ist wahrscheinlich immer noch nicht klar, warum wir dabei von einem <strong>linearen</strong> Klassifikationsmodell sprechen (die Sigmoid Funktion ist ja alles andere als linear).</p>
<p>Um zu verstehen, warum es sich um ein lineares Modell handelt, schauen wir uns nun die so genannte <strong>Decision Boundary</strong> des Modells an.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Hinweis</span>Decision Boundary
</div>
</div>
<div class="callout-body-container callout-body">
<p>Die Decision Boundary ist in einem gewissen Sinn eine <strong>Grenze</strong>. Auf der einen Seite der Grenze ist der Wertebereich der Input-Variablen <span class="math inline">\(\mathbf{x}_i\)</span>, für den <span class="math inline">\(\hat{y}_i=1\)</span> vorhergesagt wird. Auf der anderen Seite der Grenze ist der Wertebereich, für den <span class="math inline">\(\hat{y}_i=0\)</span> vorhergesagt wird. Die Decision Boundary bezieht sich also auf die Input-Variablen und zeigt uns, wo welche Vorhersagen gemacht werden.</p>
</div>
</div>
<p>Wir haben oben gesehen, dass das logistische Regressionsmodell (kompakt) wie folgt spezifiziert ist:</p>
<p><span class="math display">\[
p(y_i = 1 \mid \mathbf{x}_i) = \frac{1}{1+e^{-(\mathbf{w}' \mathbf{x_i})}}
\]</span></p>
<p>Wir schreiben dieses Modell nun etwas um, so dass wir am Schluss die Decision Boundary herleiten können. Dazu wollen wir die Gleichung so verändern, dass wir auf der linken Seite die <strong>Odds</strong> haben. Die Odds sind definiert als die <strong>Erfolgswahrscheinlichkeit</strong>, also <span class="math inline">\(p(y_i = 1 \mid \mathbf{x}_i)\)</span>, dividiert durch die Misserfolgswahrscheinlichkeit, also <span class="math inline">\(1 - p(y_i = 1 \mid \mathbf{x}_i)\)</span>.</p>
<section id="fragen" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="fragen">Fragen</h4>
<ul>
<li>Die Erfolgswahrscheinlichkeit ist 0.1. Was sind die Odds?</li>
<li>Die Odds sind 1/1 (oder 1:1). Was ist die Erfolgswahrscheinlichkeit?</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Die Erfolgswahrscheinlichkeit ist <span class="math inline">\(p=0.1\)</span>. Die Odds sind dementsprechend <span class="math inline">\(\frac{p}{1-p}=\frac{1/10}{9/10}=\frac{1}{10}\frac{10}{9}=\frac{1}{9}\)</span>. Die Odds sind also 1/9 oder 1:9.</p>
<p>Wenn die Odds 1/1 sind, dann lässt sich die Erfolgschwahrscheinlichkeit wie folgt herleiten:</p>
<p><span class="math display">\[\begin{align}
\frac{p}{1-p} &amp;= 1\\
p &amp;= 1-p\\
2p &amp;= 1\\
p &amp;= \frac{1}{2}
\end{align}\]</span></p>
</div>
</div>
</div>
<p>Die mathematische Umformung für die logistische Regression geht wie folgt:</p>
<p><span class="math display">\[\begin{align}
\frac{p(y_i = 1 \mid \mathbf{x}_i)}{1-p(y_i = 1 \mid \mathbf{x}_i)} &amp;= \frac{\frac{1}{1+e^{-(\mathbf{w}' \mathbf{x_i})}}}{1 - \frac{1}{1+e^{-(\mathbf{w}' \mathbf{x_i})}}}\\
&amp;= \frac{\frac{1}{1+e^{-(\mathbf{w}' \mathbf{x_i})}}}{\frac{1+e^{-(\mathbf{w}' \mathbf{x_i})}}{1+e^{-(\mathbf{w}' \mathbf{x_i})}} - \frac{1}{1+e^{-(\mathbf{w}' \mathbf{x_i})}}}\\
&amp;= \frac{\frac{1}{1+e^{-(\mathbf{w}' \mathbf{x_i})}}}{\frac{e^{-(\mathbf{w}' \mathbf{x_i})}}{1+e^{-(\mathbf{w}' \mathbf{x_i})}}}\\
&amp;= \frac{1}{1+e^{-(\mathbf{w}' \mathbf{x_i})}} \cdot \frac{1+e^{-(\mathbf{w}' \mathbf{x_i})}}{e^{-(\mathbf{w}' \mathbf{x_i})}}\\
&amp;= e^{(\mathbf{w}' \mathbf{x_i})}
\end{align}\]</span></p>
<p>In einem letzten Schritt können wir nun noch auf beiden Seiten den Logarithmus nehmen:</p>
<p><span class="math display">\[\begin{align}
\log\left(\frac{p(y_i = 1 \mid \mathbf{x}_i)}{1-p(y_i = 1 \mid \mathbf{x}_i)}\right) &amp;= \log\left(e^{(\mathbf{w}' \mathbf{x_i})}\right)\\
&amp;= \mathbf{w}' \mathbf{x_i}
\end{align}\]</span></p>
<p>Diese letzte Form wird <strong>Log-Odds</strong> genannt, weil wir auf der linken Seite der Gleichung nun den Logarithmus der Odds haben. Warum haben wir all das gemacht? Sie sehen, dass wir auf der rechten Seite der Gleichung nun die altbekannte <strong>lineare Form</strong> haben. Diese letzte Gleichung ist darum nun einfach zu handhaben, wenn es um die Berechnung der Decision Boundary geht.</p>
</section>
<section id="eine-input-variable" class="level4">
<h4 class="anchored" data-anchor-id="eine-input-variable">Eine Input-Variable</h4>
<p>Schauen wir uns in einem ersten Schritt an, wie die Decision Boundary aussieht, wenn wir nur eine Input-Variable haben (wie in unserem kleinen Beispiel). Die Log-Odds sehen in diesem Fall wie folgt aus:</p>
<p><span class="math display">\[
\log\left(\frac{p(y_i = 1 \mid \mathbf{x}_i)}{1-p(y_i = 1 \mid \mathbf{x}_i)}\right) = w_0 + w_1 \cdot x_{i1}
\]</span></p>
<p>Wir haben oben bereits kurz den <strong>Threshold</strong> angesprochen. Hier wählen wir einen Threshold von 50% (= 0.5). Die Decision Boundary wird die Input-Varablenwerte darstellen, die genau zu einem Modelloutput von 50% führen.</p>
<p>Wir können den Threshold-Wert in obige Gleichung einsetzen und kriegen folgendes:</p>
<p><span class="math display">\[
\begin{split}
\log\left(\frac{0.5}{1-0.5}\right) &amp;= w_0 + w_1 \cdot x_{i1}\\
\log\left(1\right) &amp;= w_0 + w_1 \cdot x_{i1}\\
0 &amp;= w_0 + w_1 \cdot x_{i1}
\end{split}
\]</span></p>
<p>Indem wir nach <span class="math inline">\(x_{i1}\)</span> auflösen, kriegen wir eine Formel, die uns den Input-Variablenwert gibt, der zu einem Modelloutput von genau 50% führt:</p>
<p><span class="math display">\[
x_{i1} = -\frac{w_0}{w_1}
\]</span></p>
</section>
<section id="frage-3" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="frage-3">Frage</h4>
<p>Wo liegt die Decision Boundary bei unserem kleinen Beispiel?</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[
x_{i1} = -\frac{w_0}{w_1} = - \frac{0.15}{0.99} \approx -0.15
\]</span> Der Input-Variablenwert, der zu einer Wahrscheinlichkeit von 0.5 führt, ist ca. -0.15. Alle <span class="math inline">\(x\)</span>-Werte kleiner als -0.15 werden als <span class="math inline">\(\hat{y}_i=0\)</span> klassifiziert, alle <span class="math inline">\(x\)</span>-Werte grösser als -0.15 als <span class="math inline">\(\hat{y}_i=1\)</span>.</p>
</div>
</div>
</div>
<p>Die folgende Abbildung zeigt die berechnete Decision Boundary auch noch grafisch:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/class-prob-db1-1.png" class="img-fluid figure-img" style="width:60.0%" alt="DB für eine Input-Variable."></p>
<figcaption>Decision Boundary für einen Threshold von 50% (bzw. 0.5) im Fall von einer Input-Variable (unser Beispiel).</figcaption>
</figure>
</div>
</div>
</div>
<p>Wenn wir also <strong>nur eine Input-Variable</strong> haben, dann ist die Decision Boundary eine <strong>Vertikale</strong> (hier am Punkt <span class="math inline">\(x=-0.15\)</span>). Wir werden später sehen, dass sich diese Vertikale je nach gewähltem Threshold verschiebt.</p>
</section>
<section id="zwei-input-variablen" class="level4">
<h4 class="anchored" data-anchor-id="zwei-input-variablen">Zwei Input-Variablen</h4>
<p>Wenn wir zwei Input-Variablen haben, dann können wir die Decision Boundary, gegeben ein Threshold <span class="math inline">\(p^*\)</span>, wie folgt herleiten:</p>
<p><span class="math display">\[\begin{align}
\log\left(\frac{p^*}{1-p^*}\right) &amp;= w_0 + w_1 \cdot x_{i1} + w_2 \cdot x_{i2}\\
w_1 \cdot x_{i1} &amp;= \log\left(\frac{p^*}{1-p^*}\right) - w_0 - w_2 \cdot x_{i2}\\
x_{i1} &amp;= \color{blue}{\frac{\log\left(\frac{p^*}{1-p^*}\right) - w_0}{w_1}} \color{red}{- \frac{w_2}{w_1}} \color{black}{\cdot x_{i2}}
\end{align}\]</span></p>
<p>Diese Form sollte Ihnen irgendwie bekannt vorkommen. Die Decision Boundary kann bei zwei Input-Variablen als <strong>Gerade</strong> im Koordinatensystem mit <span class="math inline">\(x_{i1}\)</span> auf der y-Achse und <span class="math inline">\(x_{i2}\)</span> auf der x-Achse dargestellt werden. Die Gerade hat eine Konstante (blauer Teil) und eine Steigung (roter Teil).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/class-prob-db2-1.png" class="img-fluid figure-img" style="width:60.0%" alt="DB für zwei Input-Variablen."></p>
<figcaption>Decision Boundary für einen Threshold von 50% (bzw. 0.5) im Fall von zwei Input-Variablen.</figcaption>
</figure>
</div>
</div>
</div>
<p>Die graue Gerade repräsentiert alle Kombinationen der Input-Variablen <span class="math inline">\(x_{i1}\)</span> und <span class="math inline">\(x_{i2}\)</span>, für welche das Modell eine Wahrscheinlichkeit ausspuckt, die genau dem gesetzten Threshold (hier 0.5) entspricht.</p>
<p><strong>Wichtig</strong>: wir sehen vom ein- und zweidimensionalen Beispiel, das die Decision Boundary immer <strong>linear</strong> ist. Darum gilt das logistische Regressionsmodell als einfaches Modell: es kann (im Prinzip) nur <strong>lineare Decision Boundaries</strong> fitten.</p>
</section>
<section id="lineare-separierbarkeit" class="level4">
<h4 class="anchored" data-anchor-id="lineare-separierbarkeit">Lineare Separierbarkeit</h4>
<p>Weder in unserem kleinen Beispiel mit einer Input-Variable noch im obigen Beispiel mit zwei Input-Variablen sind die Daten <strong>linear separierbar</strong>. Das heisst, es gibt keinen Threshold-Wert, mit dem die Decision Boundary die Datenpunkte perfekt separieren könnte.</p>
<p>Da die logistische Regression nur lineare Decision Boundaries modellieren kann, ist sie also dementsprechend limitiert. Jedoch gibt es, ähnlich wie bei der polynomischen Regression (<a href="02_linreg.html#sec-linregpoly" class="quarto-xref">Abschnitt&nbsp;<span>2.7</span></a>), die Möglichkeit neue Input-Variablen zu kreieren und das Modell so flexibler zu machen.</p>
<p>Was zum Beispiel mit der Decision Boundary passiert, wenn wir in dem obigen Beispiel mit zwei Input-Variablen eine dritte Input-Variable <span class="math inline">\(x_{i1}^2\)</span> ins Modell nehmen, sehen Sie in folgender Abbildung:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/class-prob-db3-1.png" class="img-fluid figure-img" style="width:60.0%" alt="DB für zwei Input-Variablen und Feature Engineering."></p>
<figcaption>Decision Boundary für einen Threshold von 50% (bzw. 0.5) im Fall von den zwei ursprünglichen Input-Variablen und einem zusätzlichen Feature <span class="math inline">\(x_{i1}^2\)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>Wow, das Modell ist jetzt schon viel flexibler. Man muss jedoch aufpassen, denn man landet so schnell im <strong>Overfitting</strong>. Dem können wir mit einer Portion Regularisierung begegnen.</p>
<!-- TODO: multi-class classification -->
</section>
</section>
</section>
<section id="naive-bayes" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="naive-bayes"><span class="header-section-number">3.2</span> Naive Bayes</h2>
<p>Ein sehr einfaches, aber oft erstaunlich gut performendes Klassifikationsmodell ist <strong>Naive Bayes</strong>. Dieses Modell beruht auf dem <strong>Satz von Bayes</strong>, den Sie vielleicht bereits kennen.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Vorsicht</span>Satz von Bayes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Bla</p>
</div>
</div>
</div>
<section id="modellspezifikation-1" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="modellspezifikation-1"><span class="header-section-number">3.2.1</span> Modellspezifikation</h3>
<p>Das Naive Bayes Modell kann folgendermassen spezifiziert werden:</p>
<p><span class="math display">\[
p(y_i = 1 \mid \mathbf{x}_i) = \frac{p(\mathbf{x}_i \mid y_i = 1) \cdot p(y_i = 1)}{p(\mathbf{x}_i)}
\]</span></p>
<p>Es lohnt sich, hier kurz zu überlegen, was die einzelnen Komponenten in dieser Modellspezifikation sind:</p>
<ul>
<li>Der linke Teil ist die Wahrscheinlichkeit, dass wir für die <span class="math inline">\(i\)</span>-te Beobachtung die Kategorie 1 beobachten, gegeben die beobachteten Input-Variablenwerte. Im Prinzip ist es derselbe Term wie der linke Teil des logistischen Regressionsmodells. Im Kontext des Naive Bayes Modell wird dieser Term bzw. diese Wahrscheinlichkeit <strong>Posterior</strong> genannt.</li>
<li>Der erste Teil rechts oben, also <span class="math inline">\(p(\mathbf{x}_i \mid y_i = 1)\)</span>, ist die Wahrscheinlichkeit der beobachteten Input-Werte der Beobachtung <span class="math inline">\(i\)</span>, gegeben sie gehört der Kategorie 1 an. Dieser Teil der Spezifikation wird <strong>Likelihood</strong> genannt.</li>
<li>Der zweite Teil rechts oben, also <span class="math inline">\(p(y_i = 1)\)</span>, ist die <em>a-priori</em> Wahrscheinlichkeit, die Kategorie 1 zu beobachten. Diese Wahrscheinlichkeit wird in der Fachsprache <strong>Prior</strong> genannt.</li>
<li>Der Nenner auf der rechten Seite ist die Wahrscheinlichkeit, die gegebenen Input-Variablenwerte zu beobachten, und zwar unabhängig davon ob die Beobachtung der Kategorie 1 oder 0 angehört. Oft wird dieser Teil <strong>Evidence</strong> oder <strong>Marginal Likelihood</strong> genannt. Mit dem <strong>Gesetz der totalen Wahrscheinlichkeit</strong> kann dieser Term berechnet werden:</li>
</ul>
<p><span class="math display">\[
p(\mathbf{x}_i) = p(\mathbf{x}_i \mid y_i = 1) \cdot p(y_i = 1) + p(\mathbf{x}_i \mid y_i = 0) \cdot p(y_i = 0)
\]</span></p>
<p>Hier sehen wir mit Naive Bayes nun zum ersten Mal ein <strong>generatives</strong> Modell (<a href="01_intro.html#sec-taxonomie" class="quarto-xref">siehe Abschnitt&nbsp;<span>1.4.3</span></a>), denn wir schätzen im Zahler der Modellspezifikation nichts anderes als die gemeinsame Verteilung von <span class="math inline">\(\mathbf{x}_i\)</span> und <span class="math inline">\(y_i\)</span>:</p>
<p><span class="math display">\[
p(\mathbf{x}_i \mid y_i) \cdot p(y_i) = p(\mathbf{x}_i,\, y_i)
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Hinweis</span>Warum dieses Modell naiv genannt wird?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Der zentrale Aspekt beim Naive Bayes Modell ist eine <strong>Unabhängigkeitsannahme</strong>. Warum brauchen wir eine Unabhängigkeitsannahme? Weil es äusserst schwierig ist, die Likelihood <span class="math inline">\(p(\mathbf{x}_i \mid y_i = 1)\)</span> aus Daten zu schätzen.</p>
<p>Nehmen wir an, wir haben 10 Input-Variablen, die alle binär sind (0/1, also zwei mögliche Werte annehmen können). In diesem Fall müssten wir aus den Daten eine <strong>gemeinsame Wahrscheinlichkeit</strong> (engl. <em>Joint Probability</em>) mit <span class="math inline">\(2^{10}=1024\)</span> möglichen Kombinationen schätzen, und zwar für jede mögliche Kategorie der Output-Variable. Etwas allgemeiner würde gelten, dass wenn wir <span class="math inline">\(p\)</span> binäre Input-Variaben haben, die gemeinsame Wahrscheinlichkeit von <span class="math inline">\(2^p\)</span> möglichen Kombinationen von Input-Variablenwerte für jede der <span class="math inline">\(C\)</span> Kategorien (hier <span class="math inline">\(C=2\)</span>) aus Daten geschätzt werden müsste.</p>
<p>In der Praxis ist das bereits ab einem kleinen <span class="math inline">\(p\)</span> unsinning: viele der möglichen Kombinationen würden im Trainingsdatensatz sehr selten oder gar nicht vorkommen, was eine robuste Schätzung dieser Wahrscheinlichkeiten verunmöglicht. Die so geschätzten Wahrscheinlichkeiten sind eine weitere Art wie sich <strong>Overfitting</strong> äussern kann.</p>
<p>Um dieses Problem zu beheben, machen wir beim Naive Bayes Modell folgende Unabhängigkeitsannahme für die Likelihood:</p>
<p><span class="math display">\[
p(\mathbf{x}_i \mid y_i = 1) = p(x_{i1} \mid y_i = 1) \cdot p(x_{i2} \mid y_i = 1) \cdot \ldots \cdot p(x_{ip} \mid y_i = 1) \cdot
\]</span></p>
<p>Anstatt die gemeinsame Wahrscheinlichkeit müssen wir hier nur die <strong>Randwahrscheinlichkeiten</strong> für jede Input-Variable separat schätzen und diese dann miteinander multiplizieren. Wir befolgen hier die bekannte Regel aus der Wahrscheinlichkeitsrechnung, dass wenn zwei Ereignisse <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> unabhängig sind, die gemeinsame Wahrscheinlichkeit wie folgt berechenbar ist:</p>
<p><span class="math display">\[
P(A, B) = P(A) \cdot P(B)
\]</span></p>
<p>Diese Unabhängigkeitsannahme ist eine starke Vereinfachung der Realität. Wenn wir beispielsweise modellieren, ob jemand zahlungsunfähig wird (<span class="math inline">\(y_i=1\)</span>) oder nicht, dann besagt die Annahmen, dass das Einkommen (gegeben es kommt zu einer Zahlungsunfähigkeit) unabhängig ist vom Beschäftigungsgrad (gegeben es kommt zu einer Zahlungsunfähigkeit). Diese beiden Input-Variablen korrelieren in der Praxis jedoch sicherlich sehr stark und sind nicht unabhängig.</p>
<p>Das erstaunliche ist jedoch, dass Naive Bayes trotz dieser Annahme in der Praxis oft gut funktioniert. Um zu verstehen, warum das so ist, hilft der <strong>Bias-Variance-Tradeoff</strong>. Die Unabhängigkeitsannahme verstärkt zwar den Bias des Modells. Gleichzeitig reduzieren wir aber die Varianz (Overfitting) substantiell, da das Modell so viel, viel einfacher wird. Oft überwiegt die Reduktion der Varianz den Anstieg im Bias-Term.</p>
</div>
</div>
</section>
<section id="modelltraining-1" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="modelltraining-1"><span class="header-section-number">3.2.2</span> Modelltraining</h3>
<p>Aus dem vorherigen Abschnitt sehen wir, dass Modelltraining im Fall von Naive Bayes heisst, dass verschiedenen Wahrscheinlichkeiten aus den Trainingsdaten geschätzt werden müssen:</p>
<ul>
<li>Die Priors <span class="math inline">\(p(y_i = 1)\)</span> und <span class="math inline">\(p(y_i = 0)\)</span></li>
<li>Die Randwahrscheinlichkeiten <span class="math inline">\(p(x_{i1} \mid y_i = 1)\)</span>, <span class="math inline">\(p(x_{i2} \mid y_i = 1)\)</span>, etc. für alle Beobachtungen im Trainingsdatensatz, die der <strong>Kategorie 1</strong> angehören.</li>
<li>Die Randwahrscheinlichkeiten <span class="math inline">\(p(x_{i1} \mid y_i = 0)\)</span>, <span class="math inline">\(p(x_{i2} \mid y_i = 0)\)</span>, etc. für alle Beobachtungen im Trainingsdatensatz, die der <strong>Kategorie 0</strong> angehören.</li>
</ul>
<p>Wir schauen uns das Modelltraining konkret für unser einfaches Klassifikationsproblem von oben mit folgender Ausgangslage an:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/class-prob-nb-1.png" class="img-fluid figure-img" style="width:60.0%" alt="Einfaches Klassifikationsbeispiel."></p>
<figcaption>Abgebildet ist die Ausgangslage eines einfachen Klassifikationsproblems mit einer Input-Variable (eingezeichnet auf der x-Achse) und der Output-Variable (eingezeichnet auf der y-Achse). In der Folge zeigen wir hier die Anwendung eines Naive Bayes Modells für dieses Problem.</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Wichtig</strong>: wir haben hier nur eine Input-Variable, weshalb die Unabhängigkeitsannahme des Naive Bayes Modells hier nicht ins Gewicht fallen wird.</p>
<p>Die Priors können ganz einfach als <strong>relative Häufigkeiten</strong> basierend auf dem Trainingsdatensatz geschätzt werden. Für unser Beispiel kriegen wir folgende Priors:</p>
<ul>
<li><span class="math inline">\(p(y_i = 1) = \frac{3}{5} = 0.6\)</span></li>
<li><span class="math inline">\(p(y_i = 0) = \frac{2}{5} = 0.4\)</span></li>
</ul>
<p>In unserem Fall haben wir eine <strong>quantitative Input-Variablen</strong>. Die Randwahrscheinlichkeiten werden für diesen Fall als Normalverteilungen, einmal für die Kategorie <span class="math inline">\(y_i=1\)</span> und einmal für die Kategorie <span class="math inline">\(y_i=0\)</span> geschätzt.</p>
<ul>
<li>Für <span class="math inline">\(y_i=0\)</span> ergibt sich die Schätzung von <span class="math inline">\(p(x_{i} \mid y_i = 0) = \mathcal{N}\left(\mu_0, \sigma_0^2\right)\)</span>, wobei
<ul>
<li><span class="math inline">\(\mu_0=\frac{1}{2}(-4.1 + 0.5) = -1.8\)</span><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></li>
<li><span class="math inline">\(\sigma_0^2 = \frac{1}{2}\left((-4.1 - (-1.8))^2 + (0.5 - (-1.8))^2\right) = 5.29\)</span><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></li>
</ul></li>
<li>Für <span class="math inline">\(y_i=1\)</span> ergibt sich die Schätzung von <span class="math inline">\(p(x_{i} \mid y_i = 1) = \mathcal{N}\left(\mu_1, \sigma_1^2\right)\)</span>, wobei
<ul>
<li><span class="math inline">\(\mu_1=\frac{1}{3}(-0.1 + 1.4 + 4.4) = 1.9\)</span></li>
<li><span class="math inline">\(\sigma_1^2 = \frac{1}{3}\left((-0.1 - 1.9)^2 + (1.4 - 1.9)^2 + (4.4 - 1.9)^2\right) = 3.5\)</span></li>
</ul></li>
</ul>
<p>Die folgende Abbildung zeigt die beiden geschätzten Normalverteilungen, die für jeden möglichen Wert von <span class="math inline">\(x\)</span> einen Dichtewert zurückgeben.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/class-prob-nb-dens-1.png" class="img-fluid figure-img" style="width:60.0%" alt="Einfaches Klassifikationsbeispiel."></p>
<figcaption>Geschätzte Dichtefunktionen (Normalverteilung) für die beiden Kategorien der Output-Variable.</figcaption>
</figure>
</div>
</div>
</div>
<p>Nun berechnen wir den Posterior <span class="math inline">\(p(y_i = 1 \mid x_i)\)</span> für den Wert <span class="math inline">\(x_i=2\)</span>:</p>
<p><span class="math display">\[\begin{align}
p(y_i = 1 \mid x_i) &amp;= \frac{p(x_i \mid y_i = 1) \cdot p(y_i = 1)}{p(x_i)}\\
&amp;= \frac{\mathcal{N}(x_i=2 \mid \mu_1, \sigma_1^2) \cdot p(y_i = 1)}{\mathcal{N}(x_i=2 \mid \mu_1, \sigma_1^2) \cdot p(y_i = 1) + \mathcal{N}(x_i=2 \mid \mu_0, \sigma_0^2) \cdot p(y_i = 0)}\\
&amp;= \frac{0.213 \cdot 0.6}{0.213 \cdot 0.6 + 0.044 \cdot 0.4}\\
&amp;= 0.88
\end{align}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03_linclass_files/figure-html/class-prob-nb-posterior-1.png" class="img-fluid figure-img" style="width:60.0%" alt="Einfaches Klassifikationsbeispiel."></p>
<figcaption>Posterior Wahrscheinlichkeiten unseres Modells für alle möglichen Werte von <span class="math inline">\(x\)</span>.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="spam-filter" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="spam-filter"><span class="header-section-number">3.2.3</span> Spam Filter</h3>
<p>Zum Schluss schauen wir uns hier (erstmal nur konzeptionell) die Anwendung des Naive Bayes Modell für einen <strong>Spam Filter</strong> an. Der Spam Filter soll eine gegebene Email möglichst korrekt in die Kategorien <strong>Spam</strong> (<span class="math inline">\(y_i=1\)</span>) oder <strong>Ham</strong> (<span class="math inline">\(y_i=0\)</span>) klassifizieren.</p>
<p>Die Trainingsdaten bestehen in diesem Fall aus <span class="math inline">\(n\)</span> Emails. Eine Sammlung von Emails sind sogenannte <strong>unstrukturierte Daten</strong> und da muss man sich überlegen, wie man die Daten in eine strukturierte Form bringt. Oder in anderen Worten, wie kommt man von einer Email zu einem Input-Vektor <span class="math inline">\(\mathbf{x}_i\)</span>? Eine Möglichkeit ist der sogenannte <strong>Bag-of-Words</strong> Ansatz. Dabei geht man folgendermassen vor:</p>
<ol type="1">
<li>Man bestimmt die <strong>häufigsten Wörter</strong> im Trainingsdatensatz (d.h., in den Emails, die wir für das Training verwenden werden).</li>
<li>Das erste Wort in diesem Set von häufig vorkommenden Wörtern wird durch die erste Input-Variable <span class="math inline">\(x_{i1}\)</span> repräsentiert, das zweite Wort durch die zweite Input-Variable <span class="math inline">\(x_{i2}\)</span>, usw. Für eine Beobachtung <span class="math inline">\(i\)</span> (bzw. die <span class="math inline">\(i\)</span>-te Email) ist die Input-Variable <span class="math inline">\(x_{i1}=1\)</span> falls das erste Wort in der Email vorkommt und sonst 0. Dasselbe gilt natürlich für alle anderen Input-Variablen. So kann eine Email in einen fixen Input-Vektor von Nullen und Einsen transformiert werden.</li>
</ol>
<p>Folgende Abbildung illustriert den Bag-of-Words Ansatz für ein ganz kleines Beispiel:</p>
<div id="fig-bagofwords" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bagofwords-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/bagOfWords.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bagofwords-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;3.1: Bag-of-Words (Beispiel)
</figcaption>
</figure>
</div>
<p>Wir schauen uns nun die Modellberechnungen anhand eines einfachen Beispiels an. Der Einfachheit halber berechnen wir hier ein Spam Filter Beispiel mit nur zwei Input-Variablen. Die beiden Variablen beschreiben, ob das Wort “afford” bzw. “price” in einer Email vorkommen oder nicht.</p>
<section id="frage-4" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="frage-4">Frage</h4>
<p>Berechnen wir als erstes die Priors. Der Trainingsdatensatz enthält insgesamt 4000 Emails. 1277 Emails sind Spam. Was sind die korrekten Priors?</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math inline">\(p(y_i = 1) = 0.32\)</span> und <span class="math inline">\(p(y_i = 0) = 0.68\)</span>.</p>
</div>
</div>
</div>
<p>Als nächstes rechnen wir die Randwahrscheinlichkeiten für die Spam Emails. In den 1277 Spam Emails enthalten 40 Emails das Wort “afford” und 223 Emails das Wort “price”.</p>
</section>
<section id="frage-5" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="frage-5">Frage</h4>
<p>Was sind die korrekten Randwahrscheinlichkeiten für die Spam Emails?</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math inline">\(p(afford_i = 1 \mid y_i = 1) = 0.03\)</span> und <span class="math inline">\(p(price_i = 1 \mid y_i = 1) = 0.17\)</span>.</p>
</div>
</div>
</div>
<p><strong>Wichtig</strong>: die Wahrscheinlichkeit, dass das Wort “afford” bzw. “price” <em>nicht</em> vorkommt in einer Spam Email haben wir so indirekt auch bereits berechnet:</p>
<p><span class="math display">\[
p(afford_i = 0 \mid y_i = 1) = 1 - p(afford_i = 1 \mid y_i = 1) = 1 - 0.03 = 0.97
\]</span></p>
<p><span class="math display">\[
p(price_i = 0 \mid y_i = 1) = 1 - p(price_i = 1 \mid y_i = 1) = 1 - 0.17 = 0.83
\]</span></p>
<p>Als letztes rechnen wir nun noch die Randwahrscheinlichkeiten für die Ham Emails. In den 2723 Ham Emails enthalten 27 Emails das Wort “afford” und 134 Emails das Wort “price”.</p>
</section>
<section id="frage-6" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="frage-6">Frage</h4>
<p>Was sind die korrekten Randwahrscheinlichkeiten für die Ham Emails?</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math inline">\(p(afford_i = 1 \mid y_i = 0) = 0.01\)</span> und <span class="math inline">\(p(price_i = 1 \mid y_i = 0) = 0.05\)</span>.</p>
</div>
</div>
</div>
<p>Auch hier können die Wahrscheinlichkeiten für das “Gegenereignis” einfach gerechnet werden:</p>
<p><span class="math display">\[
p(afford_i = 0 \mid y_i = 0) = 1 - p(afford_i = 1 \mid y_i = 0) = 1 - 0.01 = 0.99
\]</span></p>
<p><span class="math display">\[
p(price_i = 0 \mid y_i = 0) = 1 - p(price_i = 1 \mid y_i = 0) = 1 - 0.05 = 0.95
\]</span></p>
<p>Nun wollen wir die Posterior Wahrscheinlichkeit berechnen, dass eine Email, welche sowohl das Wort “afford” als auch das Wort “price” enthält, eine Spam Email ist:</p>
<p><span class="math display">\[
\begin{split}
p(y_i = 1 \mid afford_i = 1, price_i = 1) &amp;= \frac{p(afford_i = 1 \mid y_i = 1) \cdot p(price_i = 1 \mid y_i = 1) \cdot p(y_i = 1)}{p(afford_i = 1, price_i = 1)}\\
&amp;= \frac{0.03 \cdot 0.17 \cdot 0.32}{0.03 \cdot 0.17 \cdot 0.32 + 0.01 \cdot 0.05 \cdot 0.68}\\
&amp;= \frac{0.001632}{0.001632 + 0.00034}\\
&amp;= 0.83
\end{split}
\]</span></p>
<p>Wow, gegeben, dass die beiden Worte “afford” und “price” in einem Email vorkommen, sind wir also ziemlich sicher, dass es sich um eine Spam Email handelt.</p>
</section>
<section id="frage-7" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="frage-7">Frage</h4>
<p>Rechnen Sie die Wahrscheinlichkeit, dass es sich bei einer Email um Spam handelt, wenn nur das Wort “afford” vorkommt, nicht aber das Wort “price”.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\begin{split}
p(y_i = 1 \mid afford_i = 1, price_i = 0) &amp;= \frac{p(afford_i = 1 \mid y_i = 1) \cdot p(price_i = 0 \mid y_i = 1) \cdot p(y_i = 1)}{p(afford_i = 1, price_i = 0)}\\
&amp;= \frac{0.03 \cdot 0.83 \cdot 0.32}{0.03 \cdot 0.83 \cdot 0.32 + 0.01 \cdot 0.95 \cdot 0.68}\\
&amp;= \frac{0.007968}{0.007968 + 0.00646}\\
&amp;= 0.55
\end{split}
\]</span></p>
<p>Wir sind jetzt viel weniger sicher (55%), dass es sich um Spam handelt, da das Wort “price” nicht mehr in der Email vorkommt.</p>
</div>
</div>
</div>
<ul>
<li>Man kann Input-Variablen mischen.</li>
<li>Missing values</li>
<li>Works for C &gt; 2</li>
</ul>
</section>
</section>
</section>
<section id="k-nearest-neighbors" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="k-nearest-neighbors"><span class="header-section-number">3.3</span> K-Nearest Neighbors</h2>
<!-- Curse of Dimensionality -->
<!-- Optimal Bayes Classifier -->
<!-- Ch. 5 in Anil's Book -->
<p><strong>Nicht-parametrische Modelle</strong> wiederum sind Modelle, welche nicht (oder zumindest nicht explizit) durch Parameter charakterisiert sind. Am besten schauen wir uns gleich ein einfaches nicht-parametrisches Modell an, nämlich das <strong>K-Nearest-Neighbors</strong> (KNN) Modell. Stellen Sie sich vor, Sie haben einen Datensatz mit 55 Produkten aus Ihrem Sortiment. Sie haben jedes dieser 55 Produkte auf Instagram und auf Tiktok durch Influencer*innen bewerben lassen. Für jedes der 55 Produkte hatten Sie ein Werbebudget für Instagram (<span class="math inline">\(x_{i1}\)</span>) und ein Werbebudget für Tiktok (<span class="math inline">\(x_{i2}\)</span>). Am Ende des Geschäftsjahrs haben Sie für jedes der 55 Produkte bestimmt, ob die Absatzziele erreicht wurden oder nicht (Output <span class="math inline">\(y_i\)</span>). Die erfolgreichen Produkte (= Absatzziel erreicht) sind in untenstehender App als blaue Punkte eingezeichnet. Die roten Dreiecke repräsentieren die nicht-erfolgreichen Produkte. Sie sehen, dass erfolgreiche Produkte tendenziell höhere Instagram und Tiktok Werbebudgets aufwiesen als nicht-erfolgreiche Produkte. Sie möchten nun ein Modell schätzen, dass die Produkte automatisch klassifizieren kann. Dazu verwenden Sie das KNN Modell, das die <span class="math inline">\(K\)</span> nächsten Nachbarn unter den 55 gegebenen Produkten sucht und dann die häufigste Beobachtung unter den <span class="math inline">\(K\)</span> nächsten Nachbarn vorhersagt. In anderen Worten: wir suchen die <span class="math inline">\(K\)</span> <strong>ähnlichsten</strong> Beobachtungen und nutzen diese, um eine Vorhersage zu machen.</p>
<p>Selbstverständlich spielt der konkrete Wert von <span class="math inline">\(K\)</span> hier eine grosse Rolle - sollen wir nur <span class="math inline">\(K=1\)</span> Nachbarn berücksichtigen? Oder <span class="math inline">\(K=10\)</span> Nachbarn? Die erste Abbildung in der App zeigt nicht nur die 55 Datenpunkte, sondern auch die <strong>Entscheidungsgrenze</strong> (in schwarz). Untersuchen Sie kurz, wie sich diese Entscheidungsgrenze verändert, wenn Sie <span class="math inline">\(K\)</span> erhöhen oder reduzieren.</p>
<p>Ausserdem können Sie in der ersten Abbildung auch den schwarzen Punkt mit der Maus setzen, wodurch Ihnen die <span class="math inline">\(K\)</span> nächsten Punkte des schwarzen Punkts angezeigt werden.</p>
<p>Die zweite Abbildung zeigt die Entscheidungsregionen mit unterschiedlicher Intensität je nachdem wie sicher sich das Modell ist. In einer Region, in der alle <span class="math inline">\(K\)</span> Nachbarn nicht-erfolgreiche Produkte sind, sind wir uns eher sicher bezüglich der Vorhersage als in einer Region, in der die Anteile zwischen erfolgreichen und nicht-erfolgreichen Produkten ausgeglichen sind.</p>
<div class="cell">
<iframe src="https://martin-sterchi.shinyapps.io/appKNN/?showcase=0" width="672" height="600px" data-external="1">
</iframe>
</div>
<p>Um die <span class="math inline">\(K\)</span> nächsten Nachbarn zu finden, müssen wir die Distanzen zwischen Punkten rechnen können. Dazu verwenden wir die Euklidische Distanz, welche wir in Kapitel @ref(basics) kennen lernen werden.</p>
<p>Das KNN Modell ist ein sehr einfaches ML Modell, welches in der Praxis allerdings nicht allzu häufig angewendet wird. Warum nicht? Weil es am sogenannten <strong>Fluch der Dimensionalität</strong> (engl. Curse of Dimensionality) leidet. Doch was bedeutet das? Je mehr Input-Variablen wir haben, desto weiter entfernt sind Datenpunkte voneinander (das ist etwas, das man sich nur schwer vorstellen kann, aber Sie können es mir für den Moment einfach mal glauben). Das KNN beruht auf der Grundidee, dass wir <span class="math inline">\(K\)</span> nahe, ähnliche Beobachtungen für die Vorhersage verwenden. Wenn diese <span class="math inline">\(K\)</span> nahen Beobachtungen im hochdimensionalen Raum (= viele Input-Variablen) nicht mehr nahe sind, dann funktioniert auch das Modell nicht mehr gut.</p>
<section id="fragen-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="fragen-1">Fragen</h4>
<p>Stellen Sie sich vor, Sie haben folgendes Klassifikationsproblem, das Sie mit KNN lösen wollen. Welche Kategorie prognostiziert ein KNN Modell für den Punkt <span class="math inline">\(x\)</span> in der unten stehenden Abbildung?</p>
<ol type="a">
<li>Blauer Kreis.</li>
<li>Beide Klassen sind gleich wahrscheinlich.</li>
<li>Rotes Kreuz.</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Es hat drei rote Kreuze und zwei blaue Kreise in der Nachbarschaft. Die roten Kreuze sind darum in der Mehrheit, weshalb Antwort <strong>c</strong> korrekt ist.</p>
</div>
</div>
</div>
<div id="fig-knn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-knn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/knn.PNG" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;3.2: KNN Modell für binäres Klassifikationsproblem
</figcaption>
</figure>
</div>
<p>Was ist der Wert für <span class="math inline">\(K\)</span> für das KNN Modell in der oben stehenden Abbildung?</p>
<ol type="a">
<li>5</li>
<li>2</li>
<li>3</li>
<li>10</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Die Nachbarschaft, dargestellt durch den Kreis, enthält 5 Beobachtungen. Deshalb ist Antwort <strong>a</strong> korrekt.</p>
</div>
</div>
</div>
<p>Stellen Sie sich vor, Sie haben folgendes Regressionsproblem, das Sie mit KNN lösen wollen. Was ist die Vorhersage für den Punkt <span class="math inline">\(x\)</span> für das KNN-Regressionsmodell in der unten stehenden Abbildung? Die Zahlen neben den Datenpunkten stellen die entsprechenden <span class="math inline">\(y_i\)</span>-Werte dar.</p>
<ol type="a">
<li>4</li>
<li>20</li>
<li>5</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tipp</span>Lösung
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Der Durchschnitt über die 5 <span class="math inline">\(y_i\)</span>-Werte in der Nachbarschaft beträgt 4, weshalb Antwort <strong>a</strong> korrekt ist.</p>
</div>
</div>
</div>
<div id="fig-knnreg" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-knnreg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/knnreg.PNG" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knnreg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Abbildung&nbsp;3.3: KNN Modell für Regressionsproblem
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-logregR" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-logregR"><span class="header-section-number">3.4</span> Modelle in R</h2>
<p>Coming soon</p>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Manchmal wird die Kategorie 1 auch mit “Erfolg” und die Kategorie 0 mit Misserfolg betitelt. Diese Bezeichnungen sind jedoch etwas irreführend. Versucht man vorherzusagen, ob jemand eine Krankheit hat, dann ist “krank” meist die Kategorie 1. Dieses Ereignis kann kaum als “Erfolg” gewertet werden. Im Prinzip ist es Ihnen überlassen, wie Sie die beiden möglichen Kategorien mit 0/1 kodieren. Meist wird Kategorie 1 für das uns primär interessierende Ereignis gesetzt: z.B. Krankheit, Lehrabbruch, Kreditkartenbetrug.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Auch hier einmal mehr im ML eine irreführende Namensgebung: das logistische <em>Regressionsmodell</em> ist ein <em>Klassifikationsmodell</em>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Wir verwenden hier den bekannten Schätzer für den Parameter <span class="math inline">\(\mu\)</span> der Normalverteilung, nämlich <span class="math inline">\(\bar{x} = \frac{1}{n} \sum_i^n x_i\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Wir verwenden hier den bekannten Schätzer für die Varianz <span class="math inline">\(\sigma^2\)</span> der Normalverteilung, nämlich <span class="math inline">\(\hat{\sigma}^2 = \frac{1}{n} \sum_i^n (x_i - \bar{x})^2\)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Kopiert");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Kopiert");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02_linreg.html" class="pagination-link" aria-label="Lineare Regression">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Lineare Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04_pipeline.html" class="pagination-link" aria-label="ML Pipeline">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">ML Pipeline</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2026, Martin Sterchi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://www.martinsterchi.ch">
<p>https://www.martinsterchi.ch</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>